{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9f6697",
   "metadata": {},
   "source": [
    "# 웹 스크래핑 기초 (Web Scraping Basics)\n",
    "\n",
    "**수업 시간**: 3시간  \n",
    "**구성**: 강의 및 실습 2시간 + 퀴즈 1시간  \n",
    "**수준**: 중급  \n",
    "**선수 학습**: 변수, 데이터 타입, 리스트, 딕셔너리, 반복문, 함수\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 학습 목표\n",
    "\n",
    "이 수업을 마친 후 학생들은 다음을 할 수 있습니다:\n",
    "\n",
    "- 웹 스크래핑(Web Scraping)이 무엇이고 언제 사용하는지 이해하기\n",
    "- requests 라이브러리로 웹 페이지 내용 가져오기\n",
    "- BeautifulSoup으로 HTML에서 원하는 정보 추출하기\n",
    "- 간단한 웹 스크래핑 프로그램 만들기\n",
    "- 웹 스크래핑 윤리와 주의사항 알기\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 1. 웹 스크래핑이란 무엇인가?\n",
    "\n",
    "**웹 스크래핑(Web Scraping)**은 웹사이트에서 데이터를 자동으로 가져오는 기술입니다. 사람이 웹 브라우저로 하는 일을 컴퓨터가 대신 해주는 것입니다.\n",
    "\n",
    "### 실생활 비유\n",
    "온라인 쇼핑할 때 여러 사이트를 일일이 방문해서 가격을 비교하는 대신, 프로그램이 자동으로 모든 사이트를 돌아다니며 가격 정보를 모아주는 것과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a18761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동으로 하던 일\n",
    "# 1. 쿠팡 접속 → 상품 검색 → 가격 메모\n",
    "# 2. 11번가 접속 → 상품 검색 → 가격 메모  \n",
    "# 3. G마켓 접속 → 상품 검색 → 가격 메모\n",
    "# ...반복\n",
    "\n",
    "# 웹 스크래핑으로 자동화\n",
    "prices = get_all_prices(\"무선이어폰\")  # 모든 사이트에서 자동으로 가격 수집\n",
    "print(f\"총 {len(prices)}개 상품 정보를 수집했습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f162318",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 웹 스크래핑 활용 예시\n",
    "- **가격 비교**: 온라인 쇼핑몰 가격 수집\n",
    "- **뉴스 수집**: 여러 언론사 기사 모으기\n",
    "- **날씨 정보**: 기상청 데이터 가져오기\n",
    "- **취업 정보**: 구인구직 사이트 채용 공고 수집\n",
    "\n",
    "---\n",
    "\n",
    "## 📡 2. HTTP 기초 이해\n",
    "\n",
    "웹에서 정보를 주고받을 때 사용하는 규칙이 **HTTP(HyperText Transfer Protocol)**입니다.\n",
    "\n",
    "### 요청과 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브라우저가 하는 일\n",
    "# 1. 요청(Request): \"네이버 뉴스를 보여주세요\"\n",
    "# 2. 응답(Response): \"여기 HTML 파일이 있습니다\"\n",
    "\n",
    "# 파이썬에서도 같은 방식으로 작동\n",
    "response = requests.get(\"https://news.naver.com\")\n",
    "print(f\"응답 상태: {response.status_code}\")  # 200이면 성공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5510f9c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 중요한 상태 코드\n",
    "- **200**: 성공 - 정상적으로 페이지를 가져옴\n",
    "- **404**: 페이지를 찾을 수 없음\n",
    "- **403**: 접근 거부됨\n",
    "- **500**: 서버 오류\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 3. Requests 라이브러리\n",
    "\n",
    "**requests**는 파이썬에서 웹 페이지를 쉽게 가져올 수 있게 해주는 라이브러리입니다.\n",
    "\n",
    "### 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36474b1a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 웹 페이지 가져오기\n",
    "response = requests.get('https://httpbin.org/html')\n",
    "\n",
    "# 성공했는지 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"✅ 성공!\")\n",
    "    print(f\"페이지 크기: {len(response.text)} 글자\")\n",
    "else:\n",
    "    print(f\"❌ 실패: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3e209",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 예의 바르게 요청하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc028380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def polite_request(url):\n",
    "    \"\"\"서버에 부담을 주지 않는 요청\"\"\"\n",
    "    # 실제 브라우저인 것처럼 헤더 설정\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    # 1초 기다리기 (서버 부담 줄이기)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        return response\n",
    "    except:\n",
    "        print(\"요청 실패\")\n",
    "        return None\n",
    "\n",
    "# 사용 예시\n",
    "response = polite_request(\"https://httpbin.org/html\")\n",
    "if response:\n",
    "    print(\"데이터 가져오기 성공!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbeaa2d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## 🍲 4. BeautifulSoup 소개\n",
    "\n",
    "**BeautifulSoup**은 HTML에서 원하는 정보를 쉽게 찾아주는 라이브러리입니다.\n",
    "\n",
    "### 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0704ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9251bcc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 웹 페이지 가져오기\n",
    "response = requests.get('https://httpbin.org/html')\n",
    "html_content = response.text\n",
    "\n",
    "# HTML 파싱하기\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 제목 찾기\n",
    "title = soup.find('title').text\n",
    "print(f\"페이지 제목: {title}\")\n",
    "\n",
    "# 모든 링크 찾기\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(f\"링크: {link.text} → {link.get('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbce62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## 🔍 5. HTML 요소 찾기\n",
    "\n",
    "HTML은 태그로 구성되어 있습니다. BeautifulSoup으로 이런 태그들을 쉽게 찾을 수 있습니다.\n",
    "\n",
    "### 주요 HTML 태그\n",
    "- `<title>`: 페이지 제목\n",
    "- `<h1>, <h2>`: 큰 제목, 작은 제목\n",
    "- `<p>`: 문단\n",
    "- `<a>`: 링크\n",
    "- `<div>`: 구역\n",
    "\n",
    "### 태그 찾는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 샘플 HTML\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head><title>뉴스 사이트</title></head>\n",
    "<body>\n",
    "    <h1>오늘의 뉴스</h1>\n",
    "    <p>첫 번째 기사입니다.</p>\n",
    "    <p>두 번째 기사입니다.</p>\n",
    "    <a href=\"/news1\">기사 더보기</a>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 하나만 찾기\n",
    "title = soup.find('title').text\n",
    "print(f\"제목: {title}\")\n",
    "\n",
    "# 여러 개 찾기\n",
    "paragraphs = soup.find_all('p')\n",
    "for p in paragraphs:\n",
    "    print(f\"단락: {p.text}\")\n",
    "\n",
    "# class나 id로 찾기\n",
    "# <div class=\"news\">내용</div> → soup.find('div', class_='news')\n",
    "# <div id=\"main\">내용</div> → soup.find('div', id='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96087617",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## ⚖️ 6. 웹 스크래핑 윤리\n",
    "\n",
    "웹 스크래핑을 할 때 지켜야 할 규칙들이 있습니다.\n",
    "\n",
    "### 기본 원칙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1eabb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 이렇게 하세요\n",
    "import time\n",
    "\n",
    "def ethical_scraping():\n",
    "    # 1. 요청 사이에 시간 간격 두기\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 2. 적절한 헤더 사용하기\n",
    "    headers = {'User-Agent': 'Educational Bot 1.0'}\n",
    "    \n",
    "    # 3. 에러 처리하기\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response\n",
    "    except:\n",
    "        print(\"요청 실패\")\n",
    "        return None\n",
    "\n",
    "# ❌ 이렇게 하지 마세요\n",
    "# - 너무 빠르게 많은 요청 보내기\n",
    "# - 개인정보나 민감한 데이터 수집\n",
    "# - 로그인이 필요한 정보 무단 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2682e",
   "metadata": {},
   "source": [
    "### 확인해야 할 것들\n",
    "1. **robots.txt**: `사이트주소/robots.txt`에서 허용 범위 확인\n",
    "2. **이용약관**: 웹사이트 이용약관 읽기\n",
    "3. **API 존재 여부**: 공식 API가 있다면 API 사용하기\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 실습 문제\n",
    "\n",
    "### 실습 1: 간단한 뉴스 헤드라인 수집\n",
    "\n",
    "**문제**: HTML에서 뉴스 제목들을 추출해서 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 샘플 뉴스 HTML\n",
    "news_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h1>주요 뉴스</h1>\n",
    "    <h2>코스피 상승세 지속</h2>\n",
    "    <h2>새로운 기술 발표</h2>\n",
    "    <h2>날씨 맑음</h2>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 코드 작성\n",
    "soup = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "# 모든 h2 태그(뉴스 제목) 찾기\n",
    "headlines = soup.find_all('h2')\n",
    "\n",
    "print(\"=== 오늘의 뉴스 ===\")\n",
    "for i, headline in enumerate(headlines, 1):\n",
    "    print(f\"{i}. {headline.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbd8a5",
   "metadata": {},
   "source": [
    "### 실습 2: 상품 정보 추출\n",
    "\n",
    "**문제**: 쇼핑몰에서 상품명과 가격을 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9736bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 샘플 상품 HTML\n",
    "product_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div class=\"product\">\n",
    "        <h3>노트북</h3>\n",
    "        <span class=\"price\">89만원</span>\n",
    "    </div>\n",
    "    <div class=\"product\">\n",
    "        <h3>스마트폰</h3>\n",
    "        <span class=\"price\">65만원</span>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(product_html, 'html.parser')\n",
    "\n",
    "# 모든 상품 찾기\n",
    "products = soup.find_all('div', class_='product')\n",
    "\n",
    "print(\"=== 상품 목록 ===\")\n",
    "for product in products:\n",
    "    name = product.find('h3').text\n",
    "    price = product.find('span', class_='price').text\n",
    "    print(f\"상품: {name}, 가격: {price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d5f48",
   "metadata": {},
   "source": [
    "### 실습 3: 웹페이지에서 링크 수집\n",
    "\n",
    "**문제**: requests와 BeautifulSoup을 사용해서 실제 웹페이지에서 모든 링크를 수집하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def collect_links(url):\n",
    "    try:\n",
    "        # 웹페이지 가져오기\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # HTML 파싱\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 모든 링크 찾기\n",
    "            links = soup.find_all('a')\n",
    "            \n",
    "            print(f\"=== {url}에서 찾은 링크들 ===\")\n",
    "            for i, link in enumerate(links[:10], 1):  # 처음 10개만\n",
    "                href = link.get('href', '링크없음')\n",
    "                text = link.text.strip() or '텍스트없음'\n",
    "                print(f\"{i}. {text} → {href}\")\n",
    "        else:\n",
    "            print(f\"페이지를 가져올 수 없습니다: {response.status_code}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "\n",
    "# 테스트용 URL로 실행\n",
    "collect_links('https://httpbin.org/html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f621b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 퀴즈\n",
    "\n",
    "### 퀴즈 1: 기본 웹 요청\n",
    "**문제**: requests를 사용해서 'https://httpbin.org/html'의 상태 코드와 내용 일부를 출력하세요.\n",
    "\n",
    "### 퀴즈 2: HTML 파싱\n",
    "**문제**: 주어진 HTML에서 모든 `<p>` 태그의 텍스트를 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <p>첫 번째 문단</p>\n",
    "    <p>두 번째 문단</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d03617",
   "metadata": {},
   "source": [
    "### 퀴즈 3: 실전 스크래핑\n",
    "**문제**: HTML에서 제목과 링크를 추출해서 파일로 저장하세요.\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 참고 자료\n",
    "\n",
    "1. **Requests 공식 문서**: https://requests.readthedocs.io/\n",
    "2. **BeautifulSoup 가이드**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "3. **웹 스크래핑 윤리**: https://blog.apify.com/web-scraping-ethics/\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 성공을 위한 팁\n",
    "\n",
    "### 기본 원칙\n",
    "- **작은 것부터 시작**: 간단한 HTML부터 연습하기\n",
    "- **상태 코드 확인**: 항상 200인지 확인 후 진행\n",
    "- **예외 처리**: try-except로 오류 대비하기\n",
    "- **적절한 지연**: time.sleep()으로 서버 부담 줄이기\n",
    "\n",
    "### 자주 하는 실수\n",
    "- 상태 코드 확인 안 하기\n",
    "- 너무 빠르게 요청 보내기\n",
    "- HTML 구조 변경 시 대비 안 하기\n",
    "- 에러 처리 빼먹기\n",
    "\n",
    "### 연습 방법\n",
    "1. httpbin.org 같은 테스트 사이트로 연습\n",
    "2. 간단한 HTML부터 시작\n",
    "3. 점진적으로 복잡한 사이트 도전\n",
    "4. 실제 프로젝트에 적용\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 숙제\n",
    "\n",
    "### 기본 과제\n",
    "1. **나만의 뉴스 수집기**: 간단한 HTML에서 제목들을 추출하여 파일로 저장\n",
    "2. **링크 수집기**: 웹페이지의 모든 링크를 수집하여 출력\n",
    "3. **상품 정보 추출**: 쇼핑몰 HTML에서 상품명과 가격 추출\n",
    "\n",
    "### 도전 과제\n",
    "4. **실시간 정보 수집기**: \n",
    "   - 관심 있는 웹사이트에서 정보 수집\n",
    "   - 수집한 데이터를 파일로 저장\n",
    "   - 윤리적 규칙 준수하면서 구현\n",
    "\n",
    "**웹 스크래핑을 통해 데이터 세상을 탐험해보세요!** ⭐"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
